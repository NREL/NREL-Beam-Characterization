{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Characterization System Software for CSP Camera on Target System \n",
    "---\n",
    "##### Katelyn Spadavecchia, Devon Kesseli, Mackenzie Dennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob \n",
    "import glob\n",
    "import pandas as pd \n",
    "import os \n",
    "import cv2 \n",
    "from scipy.io import loadmat \n",
    "import argparse \n",
    "import imutils\n",
    "from skimage import data \n",
    "from skimage.feature import blob_dog, blob_log, blob_doh \n",
    "from math import sqrt \n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread\n",
    "from skimage.filters import gaussian \n",
    "from skimage import img_as_ubyte\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from skimage.transform import hough_line, hough_line_peaks\n",
    "from bisect import bisect \n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing image files to process\n",
    "> Software only processes one image at a time. You can choose images from specific sets by uncommenting. \n",
    "* The first image set is from Crescent Dunes \n",
    "* The second image set is at OTF from images taken by Daniel Tsvankin\n",
    "* The last three image sets are at OTF from Kyle Heinzman with target aspect ratios of 54, 75, 79. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFiles = sorted(glob.glob(\"Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/CrescentDunes/*.bmp\"),key=len)\n",
    "#imageFiles = sorted(glob.glob(\"Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/OTF.07.21.22/otf54/*.jpg\"),key=len)\n",
    "#imageFiles = sorted(glob.glob(\"Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/OTF.07.21.22/otf75/*.jpg\"),key=len)\n",
    "#imageFiles = sorted(glob.glob(\"Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/OTF.07.21.22/otf79/*.jpg\"),key=len)\n",
    "#imageFiles = sorted(glob.glob(\"Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/OTF.07.08.22/*.jpg\"),key=len)\n",
    "\n",
    "\n",
    "for i in range(len(imageFiles)):\n",
    "  print(i, \",\", imageFiles[i])\n",
    "iFile = int(input(\"Enter file number:\"))\n",
    "imageFile = imageFiles[iFile]\n",
    "print(imageFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in selected file, seperating color bands, displaying image, and printing image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in selected file\n",
    "img =imread(imageFile, as_gray = True)\n",
    "\n",
    "\n",
    "plt.imshow(img, cmap = plt.cm.gray)\n",
    "\n",
    "#splitting color bands\n",
    "hsv = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "lower_blue = np.array([40,80,0])\n",
    "upper_blue = np.array([222,222,255])\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "result = cv2.bitwise_and(img,img, mask=mask)\n",
    "r= cv2.split(result)\n",
    "g = cv2.split(result)\n",
    "b = cv2.split(result)\n",
    "r= cv2.split(img)\n",
    "g = cv2.split(img)\n",
    "b = cv2.split(img)\n",
    "\n",
    "w = img.shape\n",
    "h=img.shape\n",
    "c=img.shape\n",
    "img.dtype\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing image and improving image saturation\n",
    "> The variables alpha and beta have to be adjusted with different images. \n",
    "* If the image is well saturated: alpha=1 & beta=1\n",
    "* if the image is slightly dark: alpha=1 & beta=80\n",
    "* if the image is still dark: alpha=2 & beta=1 \n",
    "* if the image is still dark: alpha=2 & beta=80\n",
    "> Because of these adjustments, I printed out successful values for alpha & beta in a csv file under Github>DataFiles. \n",
    "This is something that will need to be automated in the future. Maybe with image intensity sorting that assigns particular alpha and beta values depending on the overall image intensities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "alpha = 2\n",
    "beta = 50\n",
    "\n",
    "im = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating binary image by applying blur, erosion, dilation, and adaptive thresholding filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Binary Image \n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "blur = cv2.blur(im, (3,3));\n",
    "erodeI = 3\n",
    "dilateI = 3\n",
    "imerode = cv2.erode(blur,kernel,iterations = erodeI)\n",
    "im_dilate = cv2.dilate(imerode,kernel,iterations = dilateI)\n",
    "bin_img = cv2.adaptiveThreshold(imerode, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 2)\n",
    "bin_img = ~bin_img\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "fileNum = iFile+1\n",
    "\n",
    "plt.imshow(bin_img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Hough Transformation. \n",
    "> The tested angles are set to look for lines that are close but not quite vertical or horizontal. This is important for intersection the Hough lines because exactly vertical lines have undefined slopes. Minimum distance is set to 150 to find unique houghpeaks in a certain space. The figure below is the hough peaks in accumulator space which is theta on the x-axis and a distance value on the y-axis. \n",
    "Since we are looking for two peaks at theta values of nearly 0, and two peaks for theta values of nearly 90, It would be helpful to search for this number of peaks in a restricted range of theta values instead of hoping the algorithm finds them correctly. In most cases, they are found correctly, but not always. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import hough_line, hough_line_peaks\n",
    "\n",
    "mintestangle = -89.03\n",
    "maxtestangle = 90.2\n",
    "\n",
    "tested_angles = np.linspace((mintestangle*np.pi)/180, (maxtestangle*np.pi)/180, 100) \n",
    "#tested_angles = np.linspace((-np.pi)/2, (np.pi)/2, 100)\n",
    "h, theta, d = hough_line(bin_img, tested_angles)\n",
    "\n",
    "\n",
    "hpeaks, angles, dists = hough_line_peaks(h,theta,d, min_distance=150, min_angle=2, threshold=150, num_peaks=4)\n",
    "#print(hpeaks.shape)\n",
    "#print(angles.shape)\n",
    "#print(dists.shape)\n",
    "#print(np.rad2deg(angles))\n",
    "#print(dists)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.log(h+1), aspect='auto', extent=[np.rad2deg(theta[0]), np.rad2deg(theta[-1]), d[-1], d[0]], cmap='gray')\n",
    "plt.plot(np.rad2deg(angles), dists, 'bo')\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting HoughLines and finding their intersection points \n",
    "> Setting $x$ and $y$ equal to get the intersection points of 2 lines. \n",
    "\n",
    "$$m_1x + b_1 = m_2x + b_2$$\n",
    "\n",
    "$$x_{int} = \\frac{b_2-b_1}{m_1-m_2}$$\n",
    "\n",
    "$$y_{int} = m_1 x_{int} +b_1 $$\n",
    "\n",
    "> The Hough Transormation orders lines based on the most amount of votes, so the order of the lines can change without being noticed. This means that some sorting had to be done in order to make sure that we were actually intersecting lines that had an intersection point (and not parallel lines). I did the sorting by finding all of the possiple intersections. There are 6 possible intersection points with 2 of them not being good, and these are very large positive numbers (since the intersection points occur are very large either positive or negative pixel values). With this knowledge, we sorted them based on their value and discarded the first two intersections. \n",
    "\n",
    "> Since the origin (0,0) is the top left corner, the largest pixel value will be the bottom right. The smallest pixel value is the top left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing.util import lineno\n",
    "plt.figure()\n",
    "plt.imshow(bin_img, cmap=plt.cm.gray)\n",
    "\n",
    "lines = np.zeros((4,4)) #save all points\n",
    "slopes = np.zeros((4,1)) #calc slopes\n",
    "y_ints = np.zeros((4,1)) #calc y-intercepts\n",
    "\n",
    "for i in range(len(hpeaks)):\n",
    "  x0 = 1\n",
    "  x1 = bin_img.shape[1] \n",
    "  y0 = (dists[i]-x0*np.cos(angles[i]))/np.sin(angles[i]) \n",
    "  y1 = (dists[i]-x1*np.cos(angles[i]))/np.sin(angles[i])\n",
    "  lines[i,:] = [x0, x1, y0, y1] #save points\n",
    "  slopes[i] = (y1-y0)/(x1-x0) #save slope\n",
    "  y_ints[i] = -slopes[i]*x0+y0 #save y-intercept\n",
    "  line = (x0,x1),(y0,y1)\n",
    "  #print(line)\n",
    "  plt.plot((x0,x1),(y0,y1), '-r')\n",
    "\n",
    "\n",
    "plt.xlim((0,bin_img.shape[1]))\n",
    "plt.ylim((bin_img.shape[0],0))\n",
    "\n",
    "def intersector(slp1, int1, slp2, int2):\n",
    "  #print(\"Slope A: \", slp1, \"Slope B :\", slp2 )\n",
    " \n",
    "  x_int = abs((int2-int1))/abs((slp1-slp2))\n",
    "  y_int = slp1*x_int+int1\n",
    "  return (x_int[0], y_int[0])\n",
    "\n",
    "#All possible intersections (Some will not yield any intersections)\n",
    "\n",
    "int_1 = intersector(slopes[0], y_ints[0], slopes[1], y_ints[1])\n",
    "int_2 = intersector(slopes[0], y_ints[0], slopes[2], y_ints[2])\n",
    "int_3 = intersector(slopes[0], y_ints[0], slopes[3], y_ints[3])\n",
    "int_4 = intersector(slopes[1], y_ints[1], slopes[2], y_ints[2])\n",
    "int_5 = intersector(slopes[1], y_ints[1], slopes[3], y_ints[3])\n",
    "int_6 = intersector(slopes[2], y_ints[2], slopes[3], y_ints[3])\n",
    "\n",
    "#sort intersections\n",
    "#There's always going to be two that don't work, these two are always going to be the first two since they are large positive numbers\n",
    "def get_max(sub):\n",
    "  return max(sub)\n",
    "\n",
    "test_list = [int_1, int_2, int_3, int_4, int_5, int_6]\n",
    "test_list.sort(key = get_max, reverse = True)\n",
    "#print(\"Sorted Tuples: \" + str(test_list))\n",
    "\n",
    "int_TL =  test_list[5]\n",
    "int_TR =  test_list[3]\n",
    "int_LL = test_list[4]\n",
    "int_LR = test_list[2]\n",
    "\n",
    "print(\"Top Left Corner: \", int_TL)\n",
    "print(\"Top Right Corner: \", int_TR)\n",
    "print(\"Lower Right Corner: \",int_LR)\n",
    "print(\"Lower Left Corner: \",int_LL)\n",
    "\n",
    "corners = [(int_TL),(int_TR), (int_LR), (int_LL)]\n",
    "x_val = [x[0] for x in corners]\n",
    "y_val = [x[1] for x in corners]\n",
    "plt.plot(x_val,y_val, 'bo')\n",
    "\n",
    "#finding the center of the target given the coordinates of the edges \n",
    "x_midpt = 0.5*(int_TL[0]+int_LR[0])\n",
    "y_midpt = 0.5*(int_TL[1] + int_LR[1])\n",
    "plt.plot(x_midpt,y_midpt, \"o\")\n",
    "target_mid = (x_midpt, y_midpt)\n",
    "print(\"Center of the Target: \",target_mid)\n",
    "\n",
    "\n",
    "#finding pixels across target\n",
    "Px = 0.5*(abs(int_TL[0] - int_TR[0]) + abs(int_LL[0] - int_LR[0]))\n",
    "Py = 0.5*(abs(int_TL[1] - int_LL[1]) + abs(int_TR[1] - int_LR[1]))\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "#plt.title(\"Detected Lines\")\n",
    "#plt.savefig('/content/drive/MyDrive/NREL/ProcessedIms/DetectedEdges/CrescentDunes/' + \"Image\" + str(fileNum) + 'Detected', dpi = 300, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the corner locations in a CSV File under Github>DataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "f = open('Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/DataFiles/CrescentDunes/TargetEdges/TargetEdges' + \"File\" + str(fileNum), 'w')\n",
    "theWriter = csv.writer(f)\n",
    "theWriter.writerow(['Image','Alpa', 'Beta','Erosions','Dilations','Top Left Corner','Top Right Corner','Lower Left Corner','Lower Right Corner'])\n",
    "theWriter.writerow([fileNum,alpha,beta,erodeI,dilateI,int_TL,int_TR,int_LL,int_LR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Characterization \n",
    "---\n",
    "##### Now that the Target corners have been detected, we can crop the image so that we are only focused on the target locations and we resize the image to the original image size, which in the case of the images at Crescent Dunes was 1936x1456. \n",
    "\n",
    "> What I'm realizing now might be better is instead of cropping the image would be to make a mask of the image so that we are focused on just the target area without changing the shape of the beam on the overall image.\n",
    "> I've cropped the image without resizing and modified the bounds to be a \"mean\" rectangle based on corner locations, rather than using a mix of x- and y-values from three corners. There may be some additional work to re-aspect the image to a proper rectangle, given that the four points will always make a trapezoid or parallelagram due to the camera-target angle. DAT 04-19-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cropping code below (DAT)\n",
    "from statistics import mean\n",
    "x_rect_left = int(mean((int_TL[0],int_LL[0])))\n",
    "x_rect_right = int(mean((int_TR[0],int_LR[0])))\n",
    "y_rect_upper = int(mean((int_TL[1],int_TR[1])))\n",
    "y_rect_lower = int(mean((int_LL[1],int_LR[1])))\n",
    "\n",
    "croppedIm = img[y_rect_upper:y_rect_lower, x_rect_left:x_rect_right]\n",
    "\n",
    "\n",
    "# Original crop + resize code below\n",
    "#croppedIm = img[int(int_TL[1]):int(int_LL[1]), int(int_LL[0]):int(int_LR[0])]\n",
    "#croppedIm = cv2.resize(croppedIm, (1936,1456))\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(croppedIm, cmap = plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Blurring, erosion, dilation canny, and adaptive thresholding filters were applied to accentuate the shape of the beam on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "blur = cv2.blur(croppedIm, (3,3))\n",
    "erodeI = 5\n",
    "dilateI = 15\n",
    "imerode = cv2.erode(blur,kernel,iterations = erodeI)\n",
    "im_dilate = cv2.dilate(imerode,kernel,iterations = dilateI)\n",
    "\n",
    "imedge = cv2.Canny(im_dilate, 8,19)\n",
    "img_th = cv2.adaptiveThreshold(imedge, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 2)\n",
    "#img_th=~img_th  \n",
    "#finding contours is easier when the contours themselves are in white. The contour I am looking for is the innermost contour that is in white. \n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "#morphlogy to remove unwanted noise\n",
    "img_th = cv2.morphologyEx(img_th, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5)))\n",
    "plt.imshow(img_th, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding contours\n",
    "> RETR_CCOMP is a good method for finding both exterior and interior contours and distinguishing them. \n",
    "\n",
    ">CHAIN_APPROX_SIMPLE is good for finding less duplicate and more unique contours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contours, heirarchy = cv2.findContours(img_th, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#img_th=~img_th\n",
    "src_copy =img_th.copy()\n",
    "src_copy = cv2.cvtColor(src_copy, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for i, cont in enumerate(contours):\n",
    "  if heirarchy[0][i][3] == -1:\n",
    "    src_copy = cv2.drawContours(src_copy, cont, -1, (0,255,0), 5)\n",
    "  else:\n",
    "    src_copy = cv2.drawContours(src_copy, cont, -1, (0,0,255),5)\n",
    "\n",
    "print(\"Numer of Contours: {}\".format(len(contours)))\n",
    "#sorted_contours = sorted(contour1, key=cv2.contourArea, reverse= True)\n",
    "#for i, cont in enumerate(sorted_contours[:3],1):\n",
    " # cv2.drawContours(img, contour1, -1, (0,255,0), 3)\n",
    "  #cv2.putText(im, str(i), (cont[0,0,1]-10), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0,255,0), 4)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.imshow(src_copy);plt.title(\"Beam Contour\");plt.axis('off')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filtering the contours, and fitting elipses to them\n",
    "> The $x$ and $y$ components of the ellispe centroid are found along with the ellipse eccentricity and are saved in a csv file under Github>DataFiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R = distance heliostat to target in meters\n",
    "# W = width of the target in meters\n",
    "# H = height of the target in meters\n",
    "R = 120\n",
    "W = 2.0 \n",
    "H = 2.2\n",
    "\n",
    "\n",
    "from numpy.ma.core import arctan\n",
    "def eccentricity_from_ellipse(contour):\n",
    "    \"\"\"Calculates the eccentricity fitting an ellipse from a contour\"\"\"\n",
    "\n",
    "    (x, y), (MA, ma), angle = cv2.fitEllipse(contour)\n",
    "\n",
    "    a = ma / 2\n",
    "    b = MA / 2\n",
    "\n",
    "    ecc = np.sqrt(a ** 2 - b ** 2) / a\n",
    "    return ecc \n",
    "\n",
    "\n",
    "\n",
    "cnts, hiers = cv2.findContours(img_th, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "src_copy =img_th.copy()\n",
    "src_copy = cv2.cvtColor(src_copy, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "max_area = 500000\n",
    "min_area = 10000\n",
    "\n",
    "\n",
    "for i, cont in enumerate(cnts):\n",
    "  if heirarchy[0][i][3] == -1:\n",
    "    src_copy = cv2.drawContours(src_copy, cont, -1, (0,255,0), 5)\n",
    "  else:\n",
    "    src_copy = cv2.drawContours(src_copy, cont, -1, (0,0,255),5)\n",
    "  \n",
    "  \n",
    "for ic,cnt in enumerate(cnts): \n",
    "  area = cv2.contourArea(cnt)\n",
    "  if min_area <= area <= max_area:\n",
    "    ellipse = cv2.fitEllipse(cnt)\n",
    "    cv2.ellipse(src_copy, ellipse, (255,0,0), 10, cv2.LINE_AA)\n",
    "    M = cv2.moments(cnt)\n",
    "    cX = int(M[\"m10\"]/M[\"m00\"])\n",
    "    cY = int(M[\"m01\"]/M[\"m00\"])\n",
    "    cv2.circle(src_copy, (cX,cY), 7, (255,0,0), -1)\n",
    "    print(\"Centroid: \",(cX,cY))\n",
    "    eccentricity = eccentricity_from_ellipse(cnt)\n",
    "    print(\"Eccentricity: \",eccentricity)\n",
    "    f1 = open('Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/DataFiles/CrescentDunes/BeamContour/BeamContour' + \"File\" + str(fileNum), 'w')\n",
    "    theWriter1 = csv.writer(f1)\n",
    "    theWriter1.writerow(['Centroids','Eccentricity'])\n",
    "    theWriter1.writerow([(cX,cY), eccentricity])\n",
    "    \n",
    "    Centroid = (cX,cY)\n",
    "    print(\"Centroid: \",(cX,cY))\n",
    "\n",
    "    #finding ∆r \n",
    "    dx = abs(Centroid[0]-target_mid[0])\n",
    "    dy = abs(Centroid[1]-target_mid[1])\n",
    "    dr = sqrt((dx)**2 +(dy)**2)\n",
    "    print(\"∆x: \",dx)\n",
    "    print(\"∆y: \", dy)\n",
    "    print(\"∆r\" , dr)\n",
    "\n",
    "    #finding the pixel extent\n",
    "    PEx = W/Px\n",
    "    PEy = H/Py\n",
    "    print(\"Pixel Extent: \", (PEx,PEy))\n",
    "\n",
    "    #finding Phi altitude \n",
    "    phi_alt = arctan(((dy*PEy)/R))\n",
    "    print(\"Altitude tracking error in meters: \",phi_alt) # in meters\n",
    "\n",
    "\n",
    "    #finding Phi elevation \n",
    "    phi_elv = arctan(((dx*PEx)/R))\n",
    "    print(\"Tracking error in Elevation: \", phi_elv) # in meters\n",
    "\n",
    "   \n",
    "\n",
    "  \n",
    "plt.figure(figsize=[5,5])\n",
    "plt.axis('off')\n",
    "plt.imshow(src_copy)\n",
    "\n",
    "plt.savefig('Y:/5700/SolarElectric/PROJECTS/38488_HelioCon_Zhu/BeamCharacterizationSystems/DataFiles/CrescentDunes/ProcessedIms/BeamDetection/' + \"Image\" + str(fileNum), bbox_inches='tight', pad_inches=0)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018f1244dc3a38e923d56fde3f449a8a7c6cb67d4778d6429d0873ab8f4f23cc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
